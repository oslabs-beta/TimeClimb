import "dotenv/config";
// eslint-disable-next-line @typescript-eslint/no-require-imports
import { Moment } from "moment";
import Bottleneck from "bottleneck";
import stepFunctionTrackersModel from "../server/models/stepFunctionTrackersModel";
import stepsModel from "../server/models/stepsModel";
import executionsObject from "./executions";
import type { Executions } from "./types";
import logs from "./logs";

import {
  CloudWatchLogsClient,
  FilterLogEventsCommand,
  FilterLogEventsCommandInput,
  FilterLogEventsCommandOutput,
  FilteredLogEvent,
} from "@aws-sdk/client-cloudwatch-logs";
import { fromEnv } from "@aws-sdk/credential-providers";
import { createTracker, Tracker } from "./Trackers";
import { createStepFunction } from "./StepFunction";

/**
 * Processed the aws api responses by grouping events by execution arn and
 * calculating the latencies for each execution.  Also adds the latency data to
 * the database and stores the hour as processed in the database.
 * @param tracker Tracker object
 * @param responses Array of responses generated by aws for a specific hour
 * @param startTime Moment start time of the hour scanned
 * @param endTime Moment end time of the hour scanned
 */
const processResponses = async (
  tracker: Tracker,
  responses: FilterLogEventsCommandOutput[],
  startTime: Moment,
  endTime: Moment
) => {
  let executions: Executions = {};
  for (const response of responses) {
    executions = await parseEvents(response.events, executions);
  }

  // calculate the average data for the step function and steps for this hour
  const [latencyData] = await logs.calculateLogLatencies(
    executions,
    tracker.stepFunction.steps
  );

  if (latencyData.executions > 0) {
    executionsObject.addLatenciesToDatabase(
      latencyData,
      tracker.trackerDbRow.step_function_id,
      tracker.stepFunction.stepIds,
      startTime,
      endTime
    );
    await tracker.updateTrackerTimes(startTime, endTime);
  }
};

/**
 * Runs the process to gather cloud watch data logs and store the results into
 * the local postegres database.
 * @returns undefined
 */
const getCloudWatchData = async (): Promise<void> => {
  console.log("Getting cloud watch data.");
  let totalRequests = 0;
  // need the tracker information to see what time period of data to retreive
  const trackerDbRows =
    await stepFunctionTrackersModel.getAllTrackerDataWithNames();
  const trackers: Tracker[] = [];
  for (const trackerDbRow of trackerDbRows) {
    // get steps for this step function
    const stepDbRows = await stepsModel.getStepsByStepFunctionId(
      trackerDbRow.step_function_id
    );
    const stepFunction = createStepFunction(stepDbRows);
    const tracker = createTracker(stepFunction, trackerDbRow);
    trackers.push(tracker);
  }

  const filteredLogsRequestBottleneck = new Bottleneck({
    maxConcurrent: 3,
    minTime: 205, // rate limit of 5 per second. added 5ms to ensure about 4.8/s
  });

  const throttledFilteredLogsRequest = async (
    tracker: Tracker,
    responses: FilterLogEventsCommandOutput[] = []
  ): Promise<void> => {
    while (true) {
      const response = await filteredLogsRequestBottleneck.schedule(
        async () => {
          const response = await getFilteredLogEvents(tracker);
          totalRequests++;
          return response;
        }
      );

      // process the result and see if we need to schedule another call here
      if (response["$metadata"].httpStatusCode === 200) {
        // all data for his hour is processed
        if (response.events === undefined || response.events.length === 0) {
          tracker.nextToken = undefined;
          responses.push(response);
          const cloneResponses = structuredClone(responses);
          const cloneStartTime = tracker.currentStartTime.clone();
          const cloneEndTime = tracker.currentEndTime.clone();
          // processed data for this hour before processing other hours
          await processResponses(
            tracker,
            cloneResponses,
            cloneStartTime,
            cloneEndTime
          );
          responses = []; // reset reponses for the next hour
          await tracker.decrementCurrentTimes();
          // see if all data for all hours have been processed
          if (await tracker.isFinished()) {
            break;
          }
        } else {
          responses.push(response);
          tracker.nextToken = response.nextToken;
        }
      } else {
        console.error("http error status code was not 200");
        break;
      }
    }
    return;
  };

  for (const tracker of trackers) {
    if (!(await tracker.isFinished())) {
      throttledFilteredLogsRequest(tracker);
    }
  }
  console.log(`Completed after making ${totalRequests} total api calls.`);
  return;
};

/**
 * Calls the FilterLogEvents api with theFilerLogEventsCommand and returns the
 * response.
 * @param tracker Tracker object
 * @returns FilterLogEventsCommandOutput object, basically the resposne from
 * aws for calling this FilterLogEvents api.
 */
const getFilteredLogEvents = async (
  tracker: Tracker
): Promise<FilterLogEventsCommandOutput> => {
  const client = new CloudWatchLogsClient({
    region: tracker.logGroupRegion,
    credentials: fromEnv(),
  });

  const params: FilterLogEventsCommandInput = {
    logGroupIdentifier: tracker.logGroupArn,
    logStreamNamePrefix: tracker.logStreamNamePrefix,
    startTime: tracker.currentStartTime.valueOf(),
    endTime: tracker.currentEndTime.valueOf(),
    nextToken: tracker.nextToken,
  };
  const command = new FilterLogEventsCommand(params);

  const response = await client.send(command);
  return response;
};

/**
 * Looks through all events and organizes them by step function
 * execution arn as described in the interface definitions. This is done to
 * effeciently gather up all of the events from the logs, which are not
 * gauranteed to be in proper order.  We also use this to parse the json stored
 * in the message property, and grab the values out we need to calculate
 * latencies between steps.
 *
 * We also process the logs this way to ensure have both a start
 * and end step to each execution, so that we only process executions which are
 * completed.  It seems the Cloudwatch logs do not gaurantee that executions are
 * completed, and either from an incomplete log perspective, or if the step
 * function has logged partial data but is still running.
 *
 * @param {FilteredLogEvent[]} events Events array from a filtered log event
 * response
 * @returns {Promise<ParsedEvent>} Promise that resolves to parsed events
 * objects, organized with keys of step functionexecution arns.
 *
 */
const parseEvents = async (
  events: FilteredLogEvent[],
  executions: Executions = {}
): Promise<Executions> => {
  for (const event of events) {
    const message = JSON.parse(event?.message);
    if (executions[message?.execution_arn] === undefined) {
      executions[message?.execution_arn] = {
        logStreamName: event.logStreamName,
        eventsFound: 0,
        events: [],
      };
    }
    executions[message?.execution_arn].events[Number(message.id) - 1] = {
      id: Number(message.id),
      type: message.type,
      name: message.details?.name,
      timestamp: Number(message.event_timestamp),
      eventId: event.eventId,
    };
    executions[message?.execution_arn].eventsFound++;
  }

  return executions;
};

// invoked to test functionality
getCloudWatchData();
